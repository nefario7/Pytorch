{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Urbansound8K in PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMf5XAzXefd8r4BqOKzc80a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nefario7/Pytorch/blob/master/Urbansound8K_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaqZ3j1hxsh4",
        "colab_type": "text"
      },
      "source": [
        "# Installing and Importing the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhXc8o8yxI6a",
        "colab_type": "code",
        "outputId": "6629f502-63e7-4a21-9a06-fcbcf7b7e4cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install torchaudio\n",
        "import os\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import librosa as lb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from torch import optim \n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/7d/8e01e21175dd2c9bb1b7e014e0c56cdd02618e2db5bebb4f52f6fdf253cb/torchaudio-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchaudio) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchaudio) (0.16.0)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpWFZFV_x1h8",
        "colab_type": "text"
      },
      "source": [
        "# Defining Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzirQqIZx8qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#*Functions to save and laod lists\n",
        "def saveList(myList, filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    np.save(filename,myList)\n",
        "    print(\"Saved successfully!\")\n",
        "def loadList(filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    tempNumpyArray=np.load(filename)\n",
        "    return tempNumpyArray.tolist()\n",
        "\n",
        "#*CUDA for PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"USING : {}\".format(device))\n",
        "\n",
        "path = os.getcwd()\n",
        "print(\"Current Working Directory : {}\".format(path))\n",
        "\n",
        "#*Paths for the data and metadata of the UrbansSound 8K Dataset\n",
        "csv_path = r\"/mnt/d/Code/UrbanSound8K/metadata\"\n",
        "file_path = r\"/mnt/d/Code/UrbanSound8K/audio\"\n",
        "csvData = pd.read_csv(os.path.join(csv_path, \"UrbanSound8K.csv\"))\n",
        "print(\"\\nMETADATA SAMPLE\")\n",
        "print(csvData.head())\n",
        "\n",
        "#*Parameters and others\n",
        "max_epochs = 100\n",
        "layer_nodes = [11479, 5000, 2000, 200, 10]\n",
        "identifiers = {0 : 'air_conditioner',\n",
        "                1 : 'car_horn',\n",
        "                2 : 'children_playing', \n",
        "                3 : 'dog_bark',\n",
        "                4 : 'drilling',\n",
        "                5 : 'engine_idling',\n",
        "                6 : 'gun_shot',\n",
        "                7 : 'jackhammer',\n",
        "                8 : 'siren',\n",
        "                9 : 'street_music'}\n",
        "\n",
        "\n",
        "#*Class defining the Custom Dataset\n",
        "class urbansound(Dataset):\n",
        "    def __init__(self, csv_path, file_path, type, transform = None):\n",
        "        self.file_path = file_path\n",
        "        self.csv_path = csv_path\n",
        "        self.file_names = []\n",
        "        self.labels = []\n",
        "\n",
        "        metadata = pd.read_csv(os.path.join(csv_path, \"UrbanSound8K.csv\"))\n",
        "        if os.path.exists(r\"/mnt/d/Code/filenames\" + type + \".npy\") and os.path.exists(r\"/mnt/d/Code/labels\" + type + \".npy\"):\n",
        "            self.file_names = loadList('filenames' + type + '.npy')\n",
        "            self.labels = loadList('labels' + type + '.npy')\n",
        "        else:\n",
        "            for i in tqdm(range(len(metadata))):\n",
        "                if metadata.iloc[i,0] in os.listdir(file_path):\n",
        "                    self.file_names.append(metadata.iloc[i, 0])\n",
        "                    self.labels.append(metadata.iloc[i, 6])\n",
        "            saveList(self.file_names, 'filenames' + type + '.npy')\n",
        "            saveList(self.labels, 'labels' + type + '.npy')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = os.path.join(self.file_path, self.file_names[index])\n",
        "        try:\n",
        "            sound, srate = torchaudio.load(path, out=None, normalization=True)\n",
        "        except FileNotFoundError or OSError:\n",
        "            print(\"\\nFile in metadata not found, continuing.\")\n",
        "\n",
        "        resound = torchaudio.transforms.Resample(srate, 44100)(sound)\n",
        "        monosound = torch.mean(resound, dim=0)\n",
        "        features = torchaudio.transforms.MFCC(sample_rate=srate, n_mfcc=13)(monosound)\n",
        "        features = features.reshape(1, -1)\n",
        "\n",
        "        reqpad = layer_nodes[0] - features.size()[1]\n",
        "        features = torch.nn.functional.pad(features, (0,reqpad), mode='constant', value=0)\n",
        "\n",
        "        return features, self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z_K2r2_yLit",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Training and Testdata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXWov3t7yKxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#*Load the Training Data \n",
        "trainset = urbansound(csv_path, os.path.join(file_path, 'all'), 'train')\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True) \n",
        "\n",
        "#*Load the Test Data\n",
        "testset = urbansound(csv_path, os.path.join(file_path, 'nntest'), 'test')\n",
        "testloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"\\nDATASET SIZES\")\n",
        "print(\"Train set size: {}\".format(len(trainset)))\n",
        "print(\"Test set size: {}\".format(len(testset)))\n",
        "\n",
        "\n",
        "#?Sample output from the dataset for verification\n",
        "# for i in range(len(testset)):\n",
        "#     sample = testset[i]\n",
        "#     print(i, sample[0], sample[1])\n",
        "\n",
        "#     if i == 4:\n",
        "#         break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atkLE5osyT-J",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoWptP_syUUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#*Definition of the Neural Network\n",
        "class neuralnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(neuralnet, self).__init__()\n",
        "        self.fc1 = nn.Linear(layer_nodes[0], layer_nodes[1])\n",
        "        self.fc2 = nn.Linear(layer_nodes[1], layer_nodes[2])\n",
        "        self.fc3 = nn.Linear(layer_nodes[2], layer_nodes[3])\n",
        "        self.fc4 = nn.Linear(layer_nodes[3], layer_nodes[4])\n",
        "        self.dropout = nn.Dropout(p=0.25)                            #? Should I add more dropouts with diff probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        output = self.dropout(F.log_softmax(self.fc4(x), dim=1))\n",
        "\n",
        "        return output\n",
        "\n",
        "test = neuralnet()\n",
        "print(\"\\nNEURAL NETWORK ARCHITECTURE\")\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0QOw7bXyYbR",
        "colab_type": "text"
      },
      "source": [
        "# Sample Outputs for verification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95IyBByzyY50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#?Sample output from Neural Net for verification\n",
        "# random_data, sampling = torchaudio.load(r\"/mnt/d/Code/UrbanSound8K/audio/all/209992-5-3-5.wav\", out=None, normalization=True)\n",
        "# res_data = torchaudio.transforms.Resample(sampling, 44100)(random_data)\n",
        "# mono_data = torch.mean(res_data, dim=0)\n",
        "# feats = torchaudio.transforms.MFCC(sample_rate=sampling, n_mfcc=13)(mono_data)\n",
        "# feats = feats.reshape(1, -1)\n",
        "\n",
        "# result = test(feats)\n",
        "# print(result)\n",
        "\n",
        "#?Sample from Dataloader\n",
        "# train_iter = iter(trainloader)\n",
        "# f, l = train_iter.next()\n",
        "# print(f.shape, l.shape)\n",
        "\n",
        "# f = f.view(f.shape[0], -1)\n",
        "# print(f.shape)\n",
        "\n",
        "# ps = torch.exp(acoustclassifier(f))\n",
        "# print(ps.shape)\n",
        "\n",
        "# top_p, top_class = ps.topk(1, dim=1)\n",
        "# print(top_p, top_class)\n",
        "# print('features shape on batch size = {}'.format(f.size()))\n",
        "# print('labels shape on batch size = {}'.format(l.size()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUJStkU6ydmc",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bI_-b8GyeEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#*Defining Loss calulcation criteria and updation parameters\n",
        "acoustclassifier = neuralnet()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(acoustclassifier.parameters(), lr = 0.001)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "#*Training Loop\n",
        "max_epochs = 10    \n",
        "print(\"\\nTRAINING\")                                    #todo: Check the no. of epochs needed\n",
        "for e in range(max_epochs):\n",
        "    runningloss = 0\n",
        "    for og_features, og_labels in tqdm(trainloader):\n",
        "        acoustclassifier.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        og_features = og_features.view(og_features.shape[0], -1)\n",
        "\n",
        "        output = acoustclassifier(og_features)\n",
        "        loss = criterion(output, og_labels)        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        runningloss += loss.item()\n",
        "        # print(\"Running Loss = {}\".format(runningloss))\n",
        "    else:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            acoustclassifier.eval()\n",
        "            print(\"\\nTESTING\")\n",
        "            for test_features, test_labels in testloader:\n",
        "                test_features = test_features.view(test_features.shape[0], -1)\n",
        "\n",
        "                logps = acoustclassifier(test_features)\n",
        "                test_loss += criterion(logps, test_labels)\n",
        "                ps = torch.exp(logps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == test_labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                \n",
        "        train_losses.append(runningloss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "\n",
        "        torch.save(acoustclassifier.state_dict(), r\"/mnt/d/Code/checkpoints/checkpoint_\" + str(e+1) + \".pth\")\n",
        "        print(\"Chkpt {} saved!\\t\".format(e+1),\n",
        "              \"Epoch: {}/{}\\t\".format(e+1, max_epochs),\n",
        "              \"Training Loss: {:.3f}\\t\".format(runningloss/len(trainloader)),\n",
        "              \"Test Loss: {:.3f}\\t\".format(test_loss/len(testloader)),\n",
        "              \"Test Accuracy: {:.3f}\\t\".format(accuracy/len(testloader)))\n",
        "\n",
        "#*Saving the trained model\n",
        "print(acoustclassifier.state_dict().keys())\n",
        "torch.save(acoustclassifier.state_dict(), r\"/mnt/d/Code/trainedmodel.pth\")\n",
        "print(\"MODEL TRAINED AND SAVED!\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}